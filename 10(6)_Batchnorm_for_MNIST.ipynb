{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-b9007a7f7abb>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\wlgh3\\venv\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\wlgh3\\venv\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\wlgh3\\venv\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\wlgh3\\venv\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\wlgh3\\venv\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Network Model Class\n",
    "    \n",
    "    Note that this class has only the constructor.\n",
    "    The actual model is defined inside the constructor.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    X : tf.float32\n",
    "        This is a tensorflow placeholder for MNIST images\n",
    "        Expected shape is [None, 784]\n",
    "        \n",
    "    y : tf.float32\n",
    "        This is a tensorflow placeholder for MNIST labels (one hot encoded)\n",
    "        Expected shape is [None, 10]\n",
    "        \n",
    "    mode : tf.bool\n",
    "        This is used for the batch normalization\n",
    "        It's `True` at training time and `False` at test time\n",
    "        \n",
    "    loss : tf.float32\n",
    "        The loss function is a softmax cross entropy\n",
    "        \n",
    "    train_op\n",
    "        This is simply the training op that minimizes the loss\n",
    "        \n",
    "    accuracy : tf.float32\n",
    "        The accuracy operation\n",
    "        \n",
    "    \n",
    "    Examples\n",
    "    ----------\n",
    "    >>> model = Model(\"Batch Norm\", 32, 10)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, name, input_dim, output_dim, hidden_dims=[32, 32], use_batchnorm=True, activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01):\n",
    "        \"\"\" \n",
    "        Constructor\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        name : str\n",
    "            The name of this network\n",
    "            The entire network will be created under `tf.variable_scope(name)`\n",
    "            \n",
    "        input_dim : int\n",
    "            The input dimension\n",
    "            In this example, 784\n",
    "        \n",
    "        output_dim : int\n",
    "            The number of output labels\n",
    "            There are 10 labels\n",
    "            \n",
    "        hidden_dims : list (default: [32, 32])\n",
    "            len(hidden_dims) = number of layers\n",
    "            each element is the number of hidden units\n",
    "            \n",
    "        use_batchnorm : bool (default: True)\n",
    "            If true, it will create the batchnormalization layer\n",
    "            \n",
    "        activation_fn : TF functions (default: tf.nn.relu)\n",
    "            Activation Function\n",
    "            \n",
    "        optimizer : TF optimizer (default: tf.train.AdamOptimizer)\n",
    "            Optimizer Function\n",
    "            \n",
    "        lr : float (default: 0.01)\n",
    "            Learning rate\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            self.X = tf.placeholder(tf.float32, [None, input_dim], name='X')\n",
    "            self.y = tf.placeholder(tf.float32, [None, output_dim], name='y')\n",
    "            self.mode = tf.placeholder(tf.bool, name='train_mode')\n",
    "            \n",
    "            #Loop over hidden layers\n",
    "            net = self.X\n",
    "            for i, h_dim in enumerate(hidden_dims):\n",
    "                with tf.variable_scope('layer{}'.format(i)):\n",
    "                    net = tf.layers.dense(net, h_dim)\n",
    "                    if use_batchnorm:\n",
    "                        net = tf.layers.batch_normalization(net, training=self.mode)\n",
    "                    net = activation_fn(net)\n",
    "            \n",
    "            #Attach fully connected layers\n",
    "            net = tf.contrib.layers.flatten(net)\n",
    "            net = tf.layers.dense(net, output_dim)\n",
    "            \n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.loss, name='loss')\n",
    "            \n",
    "            #When using the batchnormalization layers, it is necessary to manually add the update operations\n",
    "            #because the moving averages are not included in the graph\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.train_op = optimizer(lr).minimize(self.loss)\n",
    "                \n",
    "            #Accuracy etc\n",
    "            softmax = tf.nn.softmax(net, name='softmax')\n",
    "            self.accuracy = tf.equal(tf.argmax(softmax, 1), tf.argmax(self.y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.accuracy, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    \"\"\"\n",
    "    Solver class\n",
    "    \n",
    "    This class will contain the model class and session\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    model : Model class\n",
    "    sess : TF session\n",
    "        \n",
    "    Methods\n",
    "    ----------\n",
    "    train(X, y)\n",
    "        Run the train_op and Returns the loss\n",
    "        \n",
    "    evaluate(X, y, batch_size=None)\n",
    "        Returns \"Loss\" and \"Accuracy\"\n",
    "        If batch_size is given, it's computed using batch_size\n",
    "        because most GPU memories cannot handle the entire training data at once\n",
    "            \n",
    "    Example\n",
    "    ----------\n",
    "    >>> sess = tf.InteractiveSession()\n",
    "    >>> model = Model(\"BatchNorm\", 32, 10)\n",
    "    >>> solver = Solver(sess, model)\n",
    "    \n",
    "    # Train\n",
    "    >>> solver.train(X, y)\n",
    "    \n",
    "    # Evaluate\n",
    "    >>> solver.evaluate(X, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, model):\n",
    "        self.sess = sess\n",
    "        self.model = model\n",
    "    def train(self, X, y):\n",
    "        feed = {\n",
    "            self.model.X: X,\n",
    "            self.model.y: y,\n",
    "            self.model.mode: True\n",
    "        }\n",
    "        train_op = self.model.train_op\n",
    "        loss = self.model.loss\n",
    "        return self.sess.run([train_op, loss], feed_dict=feed)\n",
    "    def evaluate(self, X, y, batch_size=None):\n",
    "        if batch_size:\n",
    "            N = X.shape[0]\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            \n",
    "            for i in range(0, N, batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                feed = {\n",
    "                    self.model.X: X_batch,\n",
    "                    self.model.y: y_batch,\n",
    "                    self.model.mode: False\n",
    "                }\n",
    "                loss = self.model.loss\n",
    "                accuracy = self.model.accuracy\n",
    "                \n",
    "                step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed)\n",
    "                total_loss += step_loss * X_batch.shape[0]\n",
    "                total_acc +=step_acc * X_batch.shape[0]\n",
    "            total_loss /= N\n",
    "            total_acc /= N\n",
    "            \n",
    "            return total_loss, total_acc\n",
    "        \n",
    "        else:\n",
    "            feed = {\n",
    "                self.model.X:X,\n",
    "                self.model.y:y,\n",
    "                self.model.mode:False\n",
    "            }\n",
    "            loss = self.model.loss\n",
    "            accuracy = self.model.accuracy\n",
    "            \n",
    "            return self.sess.run([loss, accuracy], feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wlgh3\\venv\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "input_dim = 784\n",
    "output_dim = 10\n",
    "N = 55000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#We create two models: one with the batch norm and other without\n",
    "bn = Model('batchnorm', input_dim, output_dim, use_batchnorm=True)\n",
    "nn = Model('no_norm', input_dim, output_dim, use_batchnorm=False)\n",
    "\n",
    "#We create two solvers: to train both models at the same time for comparison\n",
    "#Usually we only need one solver class\n",
    "bn_solver = Solver(sess, bn)\n",
    "nn_solver = Solver(sess, nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_n = 10\n",
    "batch_size = 32\n",
    "\n",
    "#Save Losses and Accuracies every epoch\n",
    "#We are going to plot them later\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0-TRAIN] Batchnorm Loss(Acc): 0.14967(95.45%) vs No Batchnorm Loss(Acc): 0.23000(93.24%)\n",
      "[Epoch 0-VALID] Batchnorm Loss(Acc): 0.15626(95.14%) vs No Batchnorm Loss(Acc): 0.23414(93.36%)\n",
      "\n",
      "[Epoch 1-TRAIN] Batchnorm Loss(Acc): 0.10829(96.58%) vs No Batchnorm Loss(Acc): 0.19766(94.06%)\n",
      "[Epoch 1-VALID] Batchnorm Loss(Acc): 0.11915(96.18%) vs No Batchnorm Loss(Acc): 0.19944(94.08%)\n",
      "\n",
      "[Epoch 2-TRAIN] Batchnorm Loss(Acc): 0.09794(96.94%) vs No Batchnorm Loss(Acc): 0.18263(94.65%)\n",
      "[Epoch 2-VALID] Batchnorm Loss(Acc): 0.11785(96.46%) vs No Batchnorm Loss(Acc): 0.18679(94.76%)\n",
      "\n",
      "[Epoch 3-TRAIN] Batchnorm Loss(Acc): 0.07973(97.49%) vs No Batchnorm Loss(Acc): 0.23901(93.00%)\n",
      "[Epoch 3-VALID] Batchnorm Loss(Acc): 0.10185(96.82%) vs No Batchnorm Loss(Acc): 0.26578(92.64%)\n",
      "\n",
      "[Epoch 4-TRAIN] Batchnorm Loss(Acc): 0.06573(97.82%) vs No Batchnorm Loss(Acc): 0.17799(94.91%)\n",
      "[Epoch 4-VALID] Batchnorm Loss(Acc): 0.09802(96.94%) vs No Batchnorm Loss(Acc): 0.21435(94.26%)\n",
      "\n",
      "[Epoch 5-TRAIN] Batchnorm Loss(Acc): 0.07342(97.61%) vs No Batchnorm Loss(Acc): 0.17480(94.89%)\n",
      "[Epoch 5-VALID] Batchnorm Loss(Acc): 0.10670(96.82%) vs No Batchnorm Loss(Acc): 0.20619(94.24%)\n",
      "\n",
      "[Epoch 6-TRAIN] Batchnorm Loss(Acc): 0.05826(98.13%) vs No Batchnorm Loss(Acc): 0.15831(95.55%)\n",
      "[Epoch 6-VALID] Batchnorm Loss(Acc): 0.08847(97.14%) vs No Batchnorm Loss(Acc): 0.20073(94.80%)\n",
      "\n",
      "[Epoch 7-TRAIN] Batchnorm Loss(Acc): 0.05104(98.40%) vs No Batchnorm Loss(Acc): 0.15188(95.66%)\n",
      "[Epoch 7-VALID] Batchnorm Loss(Acc): 0.08904(97.22%) vs No Batchnorm Loss(Acc): 0.20139(94.66%)\n",
      "\n",
      "[Epoch 8-TRAIN] Batchnorm Loss(Acc): 0.05025(98.38%) vs No Batchnorm Loss(Acc): 0.16117(95.56%)\n",
      "[Epoch 8-VALID] Batchnorm Loss(Acc): 0.09403(97.06%) vs No Batchnorm Loss(Acc): 0.21170(94.92%)\n",
      "\n",
      "[Epoch 9-TRAIN] Batchnorm Loss(Acc): 0.04478(98.54%) vs No Batchnorm Loss(Acc): 0.15107(95.68%)\n",
      "[Epoch 9-VALID] Batchnorm Loss(Acc): 0.08847(97.36%) vs No Batchnorm Loss(Acc): 0.21492(94.90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    for _ in range(N//batch_size):\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, bn_loss = bn_solver.train(X_batch, y_batch)\n",
    "        _, nn_loss = nn_solver.train(X_batch, y_batch)       \n",
    "    \n",
    "    b_loss, b_acc = bn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    n_loss, n_acc = nn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    \n",
    "    # Save train losses/acc\n",
    "    train_losses.append([b_loss, n_loss])\n",
    "    train_accs.append([b_acc, n_acc])\n",
    "    print(f'[Epoch {epoch}-TRAIN] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
    "    \n",
    "    b_loss, b_acc = bn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    n_loss, n_acc = nn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    \n",
    "    # Save valid losses/acc\n",
    "    valid_losses.append([b_loss, n_loss])\n",
    "    valid_accs.append([b_acc, n_acc])\n",
    "    print(f'[Epoch {epoch}-VALID] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09626916, 0.9718]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_solver.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22172827, 0.9489]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_solver.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
